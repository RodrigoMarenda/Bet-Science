python
Copiar código
# Estrutura de Projeto de Machine Learning em Python

# 1. Importação das Bibliotecas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 2. Carregamento dos Dados
# Substitua 'data.csv' pelo seu arquivo de dados
data = pd.read_csv('data.csv')

# 1. Pré-processamento de dados
# Realiza a limpeza e transformação dos dados
# Exemplo de limpeza básica
data = data.dropna()  # Removendo dados ausentes, se houver
label_encoder = LabelEncoder()  # Encoding de variáveis categóricas, se necessário
data['target'] = label_encoder.fit_transform(data['target'])

# Separação entre features e target
X = data.drop('target', axis=1)
y = data['target']

# Divisão entre dados de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Padronização das features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2. Análise da Distribuição dos Dados
# Visualizações básicas para entender a distribuição e correlações
plt.figure(figsize=(10, 6))
sns.histplot(data['feature_1'], kde=True)  # Substitua 'feature_1' pelo nome da feature
plt.title("Distribuição de 'feature_1'")
plt.show()

# Matriz de correlação
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title("Matriz de Correlação")
plt.show()

# 3. Escolha do algoritmo
# Definindo o modelo inicial
model = RandomForestClassifier(random_state=42)

# Treinando o modelo
model.fit(X_train, y_train)

# 4. Otimização do modelo
# Realizando uma busca em grade para encontrar os melhores hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Melhor modelo e hiperparâmetros
best_model = grid_search.best_estimator_
print("Melhores hiperparâmetros:", grid_search.best_params_)

# 5. Valiação dos resultados
# Prevendo com os dados de teste
y_pred = best_model.predict(X_test)

# Métricas de avaliação
accuracy = accuracy_score(y_test, y_pred)
print(f"Acurácia do modelo: {accuracy:.2f}")

# Relatório de classificação
print("\nRelatório de Classificação:\n", classification_report(y_test, y_pred))

# Matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.title("Matriz de Confusão")
plt.xlabel("Predito")
plt.ylabel("Verdadeiro")
plt.show()
Explicação das Etapas
Pré-processamento de Dados:

Limpeza, transformação e divisão em conjuntos de treino e teste.
Padronização dos dados para melhorar o desempenho de alguns algoritmos.
Análise da Distribuição dos Dados:

Visualizações que ajudam a entender a distribuição dos dados e a correlação entre variáveis.
Escolha do Algoritmo:

Um modelo inicial é escolhido (nesse caso, o RandomForestClassifier), e ele é treinado com os dados de treino.
Otimização do Modelo:

Busca por hiperparâmetros usando GridSearchCV para encontrar a combinação de parâmetros que maximiza a acurácia.
Avaliação dos Resultados:

Geração de previsões, cálculo de acurácia, e exibição de métricas e visualizações de desempenho.